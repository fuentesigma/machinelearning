{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified version of ID3 algorithm\n",
    "# Original code on\n",
    "# https://towardsdatascience.com/id3-decision-tree-classifier-from-scratch-in-python-b38ef145fd90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import dequea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Entropy Measures module 2\n",
    "--------------------\n",
    "Hs: Shannon's entropy\n",
    "Hp: Non-additive superstatistical entropy\n",
    "Hm: Non-additive superstatistical complementary entropy\n",
    "\"\"\"\n",
    "def Hs(x):\n",
    "    return -x*math.log(x,2)\n",
    "\n",
    "def Hp(x):\n",
    "    return Hs(x) - 0.5*(Hs(x))**2 + 0.1666*(Hs(x))**3 - 1/24*(Hs(x))**4\n",
    "\n",
    "def Hm(x):\n",
    "    return Hs(x) + 0.5*(Hs(x))**2 + 0.1666*(Hs(x))**3 + 1/24*(Hs(x))**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Characterizes the nodes at different sublevels\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.next = None\n",
    "        self.childs = None\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Decision Tree Classifier: ID3 algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, X, feature_names, labels):\n",
    "        self.X = X\n",
    "        self.feature_names = feature_names\n",
    "        self.labels = labels\n",
    "        self.labelCategories = list(set(labels))\n",
    "        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n",
    "        self.node = None\n",
    "        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy\n",
    "\n",
    "    def _get_entropy(self, x_ids):\n",
    "        \"\"\"Entropy module\"\"\"\n",
    "        labels = [self.labels[i] for i in x_ids]\n",
    "        label_count = [labels.count(x) for x in self.labelCategories]\n",
    "        \"\"\"-----------------------------\"\"\"\n",
    "        \"\"\"Implement the entropy measure\"\"\"\n",
    "        # entropy = sum([ Hp(count/len(x_ids)) if count else 0 for count in label_count ])\n",
    "        # Renyi entropy\n",
    "        α=2.1\n",
    "        entropy = math.log(sum([ (count/len(x_ids))**α if count else 0 for count in label_count ]), 2)/(1-α)\n",
    "        \"\"\"-----------------------------\"\"\"\n",
    "        return entropy\n",
    "\n",
    "    def _get_information_gain(self, x_ids, feature_id):\n",
    "        \"\"\"Information gain module\"\"\"\n",
    "        info_gain = self._get_entropy(x_ids)\n",
    "        x_features = [self.X[x][feature_id] for x in x_ids]\n",
    "        feature_vals = list(set(x_features))\n",
    "        feature_vals_count = [x_features.count(x) for x in feature_vals]\n",
    "        feature_vals_id = [\n",
    "            [x_ids[i]\n",
    "            for i, x in enumerate(x_features)\n",
    "            if x == y]\n",
    "            for y in feature_vals\n",
    "        ]\n",
    "\n",
    "        info_gain = info_gain - sum([val_counts / len(x_ids) * self._get_entropy(val_ids)\n",
    "                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n",
    "        \"\"\"Searching for features that maximize the information gain\"\"\"\n",
    "        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n",
    "        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n",
    "\n",
    "        return self.feature_names[max_id], max_id\n",
    "\n",
    "    def id3(self):\n",
    "        \"\"\"Initialize ID3 algorithm\"\"\"\n",
    "        x_ids = [x for x in range(len(self.X))]\n",
    "        feature_ids = [x for x in range(len(self.feature_names))]\n",
    "        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n",
    "        print('')\n",
    "\n",
    "    def _id3_recv(self, x_ids, feature_ids, node):\n",
    "        \"\"\"ID3 algorithm\"\"\"\n",
    "        if not node:\n",
    "            node = Node()\n",
    "        labels_in_features = [self.labels[x] for x in x_ids]\n",
    "        if len(set(labels_in_features)) == 1:\n",
    "            node.value = self.labels[x_ids[0]]\n",
    "            return node\n",
    "        if len(feature_ids) == 0:\n",
    "            node.value = max(set(labels_in_features), key=labels_in_features.count)\n",
    "            return node\n",
    "        # else...\n",
    "        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n",
    "        node.value = best_feature_name\n",
    "        node.childs = []\n",
    "        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n",
    "        for value in feature_values:\n",
    "            child = Node()\n",
    "            child.value = value\n",
    "            node.childs.append(child)\n",
    "            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n",
    "            if not child_x_ids:\n",
    "                child.next = max(set(labels_in_features), key=labels_in_features.count)\n",
    "                print('')\n",
    "            else:\n",
    "                if feature_ids and best_feature_id in feature_ids:\n",
    "                    to_remove = feature_ids.index(best_feature_id)\n",
    "                    feature_ids.pop(to_remove)\n",
    "                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n",
    "        return node\n",
    "\n",
    "    def printTree(self):\n",
    "        if not self.node:\n",
    "            return\n",
    "        nodes = deque()\n",
    "        nodes.append(self.node)\n",
    "        while len(nodes) > 0:\n",
    "            node = nodes.popleft()\n",
    "            print(node.value)\n",
    "            if node.childs:\n",
    "                for child in node.childs:\n",
    "                    print('({})'.format(child.value))\n",
    "                    nodes.append(child.next)\n",
    "            elif node.next:\n",
    "                print(node.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate syntetic data\n",
    "data = {\n",
    "    # attributes\n",
    "    'social_network': ['FB', 'TW', 'IG', 'TK'],\n",
    "    'suspicious': ['Low', 'High'],\n",
    "    'viralization': ['small', 'medium', 'large'],\n",
    "    # target\n",
    "    'fake_news': ['Yes', 'No']\n",
    "}\n",
    "\n",
    "data_df = pd.DataFrame(columns=data.keys())\n",
    "# vary the number of instances to examine the asymptotic behaviour of entropy measures\n",
    "np.random.seed(42)\n",
    "for i in range(1000):\n",
    "    data_df.loc[i, 'social_network'] = str(np.random.choice(data['social_network'], 1)[0])\n",
    "    data_df.loc[i, 'suspicious'] = str(np.random.choice(data['suspicious'], 1)[0])\n",
    "    data_df.loc[i, 'viralization'] = str(np.random.choice(data['viralization'], 1)[0])\n",
    "    data_df.loc[i, 'fake_news'] = str(np.random.choice(data['fake_news'], 1)[0])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical experiments\n",
    "# testing 5 classifications in a row to measure the average running-time\n",
    "# ----------------------------------------------------------------------/\n",
    "\n",
    "# separate target from predictors\n",
    "X = np.array(data_df.drop('fake_news', axis=1).copy())\n",
    "y = np.array(data_df['fake_news'].copy())\n",
    "feature_names = list(data_df.keys())[:3]\n",
    "\n",
    "# time stamps\n",
    "intervals = np.zeros(5)\n",
    "\n",
    "# evaluations\n",
    "for i in range(5):\n",
    "    # run algorithm -----------------------------------/\n",
    "    t = time.process_time()\n",
    "    # call algorithm ID3\n",
    "    tree_clf = DecisionTreeClassifier(X=X, feature_names=feature_names, labels=y)\n",
    "    print(\"System entropy {:.4f}\".format(tree_clf.entropy))\n",
    "    # run algorithm\n",
    "    tree_clf.id3()\n",
    "    tree_clf.printTree()\n",
    "    # end algorithm -----------------------------------/\n",
    "    intervals[i] = time.process_time() - t\n",
    "    \n",
    "print(intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
